{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5f015e03d6b67689066e62eb84cff05",
     "grade": false,
     "grade_id": "cell-0e5bc97936ef37b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Model Selection and Evaluation - Part 2\n",
    "\n",
    "## Activity  3 - Cross Validation\n",
    "\n",
    "In the previous activity, we made the classic mistake of usin our test dataset to tune our model's\n",
    "hyperparamters and to predict our generalization error. This tends to give misleadingly optimistic predictions about how well we will do on unobserved data. Remember that we carefully picked our hyperparameter values to do as well as possible *on our held-out data*. We shouldn't be surprised when our model performs better on that data than on unobserved data. This problem is particularly acute if our data set is small.\n",
    "\n",
    "The traditional solution is to divide our data into three disjoint sets: **training**, **validation**, and **testing**:\n",
    "* The **training** set is used to fit the model.\n",
    "* The **validation** set is used to evaluate models for the purpose of hyperparameter selection. \n",
    "* The **test** set is kept in a locked room guarded by jaguars. We only look at the testing set ONCE, when we have finalized our model. That way our performance on the test set gives us an unbiased estimate of our generalization error.\n",
    "\n",
    "This traditional approach is fine if we have a lot of data to work with. If the data set is small, we are faced with a painful dilemma: More validation data means better model selection. More testing data means more accurate model evaluation. More training data means better models. Any data we use for one purpose can't be used for the others.\n",
    "\n",
    "**Cross validation** is one way to use limited data more effectively.  The cells below walk us through an example of using cross validation for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "adf6c0bd287456de7b4681745d46ad28",
     "grade": false,
     "grade_id": "cell-6184a7c96683b3c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from DTClassBounds import decision_areas, plot_areas  \n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "dataset_size = 200\n",
    "X, y = make_moons(n_samples=dataset_size, noise=0.3, random_state=0)\n",
    "\n",
    "# Split our data into a training and testing set...\n",
    "split_point = int(X.shape[0] * .8) # Use 80% of the data to train the model\n",
    "\n",
    "X_train = X[0:split_point, :]\n",
    "y_train = y[0:split_point]\n",
    "\n",
    "X_test = X[split_point::, :] # This data will ONLY be used for final evaluation.\n",
    "y_test = y[split_point::]\n",
    "\n",
    "print('training and test creation complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b04a808429f5de098cbe5a2ad58d62e3",
     "grade": false,
     "grade_id": "cell-a25e5329c4fd6ae6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The following cell shows how we can use the scikit-learn `KFold` class to automatically split up our **training** data for k-fold cross validation. Take a minute to read through this code to make sure you understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "FOLDS = 10\n",
    "MAX_LEAVES = 80\n",
    "kf = KFold(n_splits=FOLDS)\n",
    "\n",
    "error_rates = np.zeros((FOLDS, MAX_LEAVES - 2)) # (can't have 0 or 1 leaves)\n",
    "\n",
    "# Loop over all of the hyperparameter settings\n",
    "for max_leaves in range(2, MAX_LEAVES):\n",
    "    \n",
    "    k = 0\n",
    "    # Evaluate each one K-times\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_index], X_train[val_index]\n",
    "        y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        tree = DecisionTreeClassifier(max_leaf_nodes=max_leaves)\n",
    "        tree.fit(X_tr, y_tr)\n",
    "        \n",
    "        y_val_predict = tree.predict(X_val)\n",
    "        error_rates[k, max_leaves - 2] = np.sum((y_val - y_val_predict)**2) / y_val.size\n",
    "        \n",
    "        k += 1\n",
    "        \n",
    "# Average across the k folds\n",
    "error_rates_avg = np.mean(error_rates, axis=0)\n",
    "\n",
    "plt.plot(np.arange(2, MAX_LEAVES), error_rates_avg)\n",
    "plt.xlabel('max leaves', fontsize=16)\n",
    "plt.ylabel('Error Rate(%)',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef305fb1fce301500cd2823b3fc3a162",
     "grade": false,
     "grade_id": "cell-310b0088d97a89a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# <span style=\"color:red\">Question 10</span>\n",
    "\n",
    "What is contained in the variables *train_index* and *val_index* during each loop \n",
    "iteration?  What is kf.split returning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "632e271d88c377d46613f35b9a930de4",
     "grade": true,
     "grade_id": "cell-64be48244763e093",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## *YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation with Scikit-Learn\n",
    "\n",
    "Scikit-learn offers a framework to automate some of this by using the `cross_val_score` function:  \n",
    "\n",
    "(There are also library routines for [automating the entire process of hyperparameter tuning](https://scikit-learn.org/stable/modules/grid_search.html).) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 2d matrix, each fold has a row and each leaf setting is a column\n",
    "error_rates = np.zeros((FOLDS, MAX_LEAVES - 2))\n",
    "\n",
    "# Loop over all of the hyperparameter settings\n",
    "for size in range(2, MAX_LEAVES):\n",
    "    tree = DecisionTreeClassifier(max_leaf_nodes=size)\n",
    "    \n",
    "    # Returns an array of cross validation results.\n",
    "    # NOTE: 1 - accuracy is the error_rate\n",
    "    error_rates[:, size - 2] = 1 - cross_val_score(tree, X_train, y_train, \n",
    "                                         cv=FOLDS, scoring='accuracy')\n",
    "    \n",
    "error_rate_avg = np.mean(error_rates, axis=0)\n",
    "smallds_error_rate_avg = error_rate_avg\n",
    "model_plot_200 = plt.plot(np.arange(2, MAX_LEAVES), error_rate_avg)\n",
    "\n",
    "plt.xlabel('max leaves', fontsize=16)\n",
    "plt.ylabel('Error Rate(%)',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b5898e71155eefe8e83b2e7a3301a0c",
     "grade": false,
     "grade_id": "cell-0f3de0621c4a7ff8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# <span style=\"color:red\">Question 11</span>\n",
    "\n",
    "\n",
    "* Based on the results above, what is the most promising hyperparameter value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Answer 11</span>\n",
    "\n",
    "* YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Data Please\n",
    "\n",
    "The plot of the cross-validation error rate did not make our decision of the hyperparameter easy.\n",
    "In more complex applications of ML, this becomes much more difficult, especially when a model has many\n",
    "different hyperparameters to tune and the models take significant time to construct.  \n",
    "\n",
    "Let''s see if the choice of the single hyperparameter in this example becomes easier if\n",
    "we just had more data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_large, y_large = make_moons(n_samples=1000, noise=0.3, random_state=0)\n",
    "\n",
    "# Split our data into a training and testing set...\n",
    "split_point = int(X_large.shape[0] * .8) # Use 80% of the data to train the model\n",
    "\n",
    "X_large_train = X_large[0:split_point, :]\n",
    "y_large_train = y_large[0:split_point]\n",
    "\n",
    "X_large_test = X_large[split_point::, :] \n",
    "y_large_test = y_large[split_point::]\n",
    "\n",
    "\n",
    "\n",
    "# 2d matrix, each fold has a row and each leaf setting is a column\n",
    "error_rates = np.zeros((FOLDS, MAX_LEAVES - 2))\n",
    "\n",
    "# Loop over all of the hyperparameter settings\n",
    "for size in range(2, MAX_LEAVES):\n",
    "    tree = DecisionTreeClassifier(max_leaf_nodes=size)\n",
    "    \n",
    "    # Returns an array of cross validation results.\n",
    "    # NOTE: 1 - accuracy is the error_rate\n",
    "    error_rates[:, size - 2] = 1 - cross_val_score(tree, X_large_train, \n",
    "                                                   y_large_train, \n",
    "                                         cv=FOLDS, scoring='accuracy')\n",
    "    \n",
    "error_rate_avg = np.mean(error_rates, axis=0)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(np.arange(2, MAX_LEAVES), error_rate_avg,c = 'blue',\n",
    "                   label='dataset ' + str(X_large_train.shape[0]))\n",
    "plt.plot(np.arange(2, MAX_LEAVES), smallds_error_rate_avg, c = 'red',\n",
    "                   label='dataset ' + str(X_train.shape[0]))\n",
    "\n",
    "\n",
    "plt.xlabel('max leaves', fontsize=16)\n",
    "plt.ylabel('Error Rate(%)',fontsize=16)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a value for our hyperparameter, let's train our final model on the *full* training set and use our locked-away testing set to predict model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2dd9765defa980d3047c571331703543",
     "grade": true,
     "grade_id": "cell-0cfbfe28ccc93c86",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Students -- modify code here\n",
    "\n",
    "predictGenErrortree = DecisionTreeClassifier(max_leaf_nodes=???) # Put your best hyperparameter here!\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Train using ALL the training data (smaller set)\n",
    "predictGenErrortree.fit(X_train, y_train)\n",
    "\n",
    "# Test on held-out testing data\n",
    "y_test_predict = predictGenErrortree.predict(X_test)\n",
    "predicted_gen_error_rate = np.sum((y_test - y_test_predict)**2) / y_test.size\n",
    "\n",
    "print(\"Predicted generalization error rate: {:.4f}\".format(predicted_gen_error_rate ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1832aace2a4e6cb49eaa47ac469bc67c",
     "grade": false,
     "grade_id": "cell-b37111e7ec81b474",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Did it work?\n",
    "\n",
    "Since none of the data we are testing on here was used *in any way* to design or fit the model, this value should give us an unbiased estimate of our generalization error.  Let's try testing on some new unobserved data to see how good our estimate is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's see how we do on unobserved data... \n",
    "X_new, y_new = make_moons(n_samples=500, noise=0.3, random_state=4)\n",
    "y_new_predict = predictGenErrortree.predict(X_new)\n",
    "error_rate = np.sum((y_new - y_new_predict)**2) / y_new.size\n",
    "print(\"error rate: {:.4f}\".format(error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0db0813d44882afa78fe82643cb8c51",
     "grade": false,
     "grade_id": "cell-e71e38a96b985176",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# <span style=\"color:red\">Question 12</span>\n",
    "\n",
    "\n",
    "Take a look back at the results from Exercise 2 and answer the following questions:\n",
    "\n",
    "* Did the cross validation approach improve our results in terms of model selection (i.e. did we end up with a better model)?  Justify your answer.\n",
    "* Did maintaining a proper test set improve our results in terms of model evaluation (i.e. did we make a more accurate prediction about our generalization error)?  Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Answer 12 (two parts)</span>\n",
    "\n",
    "\n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4 - Nested Cross Validation\n",
    "\n",
    "There is still one drawback of our approach from the previous exercise:  Our testing set was quite small, which limits the reliability of our model evaluation. Maybe the test set happens to contain particularly difficult or particularly easy instances? \n",
    "\n",
    "In a perfect world, we would like to be able to use *all* of our data for training and validation and *all* of our data for testing.  In a way, we can accomplish this by using **nested cross validation**.  This involves generating K-fold data splits at two levels. At the top level, we iterate over Testing/Training splits for the purpose of model evaluation.  For *each* Training/Testing split we iterate over Training/Validation splits for the purpose of model tuning.  This page provides an [example of nested cross-validation using scikit-learn](https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html). \n",
    "\n",
    "Although you are required to understand nested k-fold cross validation, it is optional if you wish to apply nested cross validation to redo our example above.  Does it improve our estimate of generalization error? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
