{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07c8014632d3c9c7b5eb090b9f9777e3",
     "grade": false,
     "grade_id": "cell-a344ab152bceb18e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Model Selection and Evaluation - Part 1\n",
    "\n",
    "When building supervised machine learning models, we need to solve two problems:\n",
    "\n",
    "1. **Model selection** - Finding the model that does as well as possible\n",
    "on our learning task.\n",
    "\n",
    "2. **Model evaluation** - Predicting **generalization error**, or the expected performance of our\n",
    "model on unseen data.\n",
    "\n",
    "Both are critical.  Without 1. we can't have an effective model and without 2. we can't *know* if we have an effective model.\n",
    "\n",
    "Once we have picked a particular machine learning algorithm, model\n",
    "selection comes down to the problem of **hyperparameter** tuning.\n",
    "Hyperparameters are parameters of our learning models that need to be\n",
    "selected before the model can be learned.  Examples include the\n",
    "maximum number of leaves in a decision tree or the number of hidden\n",
    "units in a neural network classifier.  The **parameters** of our model\n",
    "are learning from the training data, for example, the feature and split point\n",
    "to use to for a node in a decision tree.  Parameters are learned from the training data,\n",
    "and the hyperparameters guide the learning process.  \n",
    "\n",
    "**WARNING:**  The below activities showcase several *BAD* approaches to model selection and evaluation.  These examples are not meant to illustrate the correct way of doing things, they are meant to show the consequences of doing things incorrectly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9b812cafad7cb8daea6e08301983f2d",
     "grade": false,
     "grade_id": "cell-0b56c46646e7be58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Activity 1 - Naive Model Selection\n",
    "\n",
    "For now, let's focus entirely on model selection and disregard model\n",
    "evaluation.  The following cell will load a data set and use a\n",
    "decision tree classifier to fit a decision tree to the data. Try adjusting the `max_leaf_nodes` hyperparameter in order to minimize the error rate on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tommy\\Desktop\\CS445\\lab2\\model_selection_classification_1.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tommy/Desktop/CS445/lab2/model_selection_classification_1.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tommy/Desktop/CS445/lab2/model_selection_classification_1.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tommy/Desktop/CS445/lab2/model_selection_classification_1.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mDTClassBounds\u001b[39;00m \u001b[39mimport\u001b[39;00m decision_areas,plot_areas\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from DTClassBounds import decision_areas,plot_areas\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.3, random_state=0)\n",
    "# Build a decision tree regressor\n",
    "\n",
    "tree = DecisionTreeClassifier(max_leaf_nodes=27)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Evaluate the error rate of our decision tree on the data set\n",
    "y_predict = tree.predict(X)\n",
    "error_rate = np.sum((y - y_predict)**2) / y.size\n",
    "print(\"error rate: {:.4f}\".format(error_rate))\n",
    "\n",
    "# Visualize the decision boundaries\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=cm_bright)\n",
    "rec = decision_areas(tree, [-2,3,-2,3])\n",
    "plot_areas(rec)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b56a00712a453d0a8651f7d2843086e",
     "grade": false,
     "grade_id": "cell-2e1a8dda8aed561a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# <span style=\"color:red\">Question #1</span>\n",
    "\n",
    "* What value of the hyperparameter resulted in the lowest error rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97dd0b302621948f68b735ca62b9973f",
     "grade": true,
     "grade_id": "cell-bfa3abddf6a65c1c",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## *YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Question #2</span>\n",
    "\n",
    "Do you think that this error rate reflects how well this model will do on unseen data?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4be5458d44c2cd98b512162faba1169e",
     "grade": true,
     "grade_id": "cell-7edd77275ac77e02",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## *YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Data Arrives!\n",
    "\n",
    "In the exercise above, you tuned the hyperparameters to achieve the *best* fit the training data.  Now let's see what happens when we use this model on some new data drawn from the same underlying distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = make_moons(n_samples=200, noise=0.3, random_state=1)\n",
    "\n",
    "print('Using your best model, which was fit in a prior cell with {} leaves'\n",
    "     .format(tree.get_n_leaves()))\n",
    "\n",
    "y_test_predict = tree.predict(X_test)\n",
    "error_rate = np.sum((y_test - y_test_predict)**2) / y_test.size\n",
    "\n",
    "print(\"error rate: {:.4f}\".format(error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2 - Using a Test Set for Hyperparameter Tuning and Evaluation\n",
    "\n",
    "In the exercise above, you were able to perfectly fit a training data set, but that didn't tell you anything about how well your model would perform on unseen data.  Recall that we want our models to **generalize**, that is, perform well on data that the model has not seen\n",
    "previously. \n",
    "\n",
    "One approach to address this is by splitting our limited data into a **training set** and a **test** set.  This is illustrated in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.3, random_state=0)\n",
    "\n",
    "# Split our data into a training and testing set...\n",
    "split_point = int(X.shape[0] * .8) # Use 80% of the data to train the model\n",
    "\n",
    "X_train = X[0:split_point, :]\n",
    "y_train = y[0:split_point]\n",
    "\n",
    "X_test = X[split_point::, :]\n",
    "y_test = y[split_point::]\n",
    "\n",
    "# Build a decision tree regressor using the TRAINING set\n",
    "tree = DecisionTreeClassifier(max_leaf_nodes=3)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the error rate of our decision tree on the TESTING set \n",
    "y_test_predict = tree.predict(X_test)\n",
    "error_rate= np.sum((y_test - y_test_predict)**2) / y_test.size\n",
    "\n",
    "print(\"test data error rate: {:.4f}\".format(error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Student Activity (Question #3)</span>\n",
    "\n",
    "In the below code block, write a loop that iterates over the hyperparameter for\n",
    "the number of leaves and reports the value that produces the lowest error rate\n",
    "on the **test** data.  \n",
    "\n",
    "Since this class focuses on visualization and explaining the results,\n",
    "for each hyperparameter setting, store the error rate for\n",
    "the training and test sets in 2 separate python lists.  You should be\n",
    "able to deduce the number of leaves used by the position in the list\n",
    "(the first position used 2 leaves, the second position in the list used\n",
    "3 leaves, ...).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "625781e612617b91525c79b22c643086",
     "grade": false,
     "grade_id": "cell-fd99b32e606cdaeb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer to question 3 goes here\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.3, random_state=0)\n",
    "\n",
    "# Split our data into a training and testing set...\n",
    "split_point = int(X.shape[0] * .8) # Use 80% of the data to train the model\n",
    "\n",
    "X_train = X[0:split_point, :]\n",
    "y_train = y[0:split_point]\n",
    "\n",
    "X_test = X[split_point::, :]\n",
    "y_test = y[split_point::]\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "\"\"\"\n",
    "Code a loop that varies the hyperparameter for the\n",
    "number of leaves in the tree and records BOTH\n",
    "the error rate on the training data and the \n",
    "error rate on the test data.  Vary the number\n",
    "of leaves from 2 to the number of data points.\n",
    "\"\"\"\n",
    "\n",
    "train_error_rates = [0] * (X_train.shape[0] - 1)\n",
    "test_error_rates =  [0] * (X_train.shape[0] - 1)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "## Find the best value (NOT using a for loop)\n",
    "## The BEST is the lowest test error rate with the fewest leaves\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84f63533c083d1b8161a2b7d015c15b6",
     "grade": true,
     "grade_id": "cell-8a0510f094466364",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is code to test your answer to Question 3\n",
    "# just run it after running your code and see how your answers compares\n",
    "assert(len(train_error_rates) == (X_train.shape[0] -1))\n",
    "\n",
    "# test the first and last values \n",
    "assert(train_error_rates[0] == 0.1875)\n",
    "assert(test_error_rates[0] == 0.15)\n",
    "\n",
    "assert(train_error_rates[-1] == 0.0)\n",
    "assert(test_error_rates[-1] == 0.175)\n",
    "\n",
    "\n",
    "train_error_rates_ref = [0.1875, 0.1375, 0.1125, 0.1125, 0.1125, 0.08125, 0.08125,\n",
    "                     0.06875, 0.0625, 0.05625, 0.05, 0.05, 0.05, 0.05, 0.0375,\n",
    "                     0.03125, 0.025, 0.025, 0.0125, 0.0125, 0.00625, 0.00625, \n",
    "                     0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
    "                     0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
    "                     0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
    "                     0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
    "                     0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
    "                     0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
    "                     0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
    "                     0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
    "                     0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
    "                     0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
    "                     0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "print(int(len(train_error_rates_ref)))\n",
    "\n",
    "test_error_rates_ref = [0.15, 0.1, 0.075, 0.075, 0.075, 0.175, 0.175, 0.175, 0.075, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, \n",
    "                        0.175, 0.175, 0.175, 0.175, 0.175]\n",
    "\n",
    "np.testing.assert_array_equal(train_error_rates_ref, train_error_rates,\n",
    "                                      err_msg='Test error rate array mismatch')\n",
    "np.testing.assert_array_equal(test_error_rates_ref, test_error_rates,\n",
    "                                      err_msg='Test error rate array mismatch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa16f37e19a439ee7befee3241819f46",
     "grade": false,
     "grade_id": "cell-4f8fa651d4cea124",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# <span style=\"color:red\">Question #4</span>\n",
    "\n",
    "- What hyperparameter settings gives us the lowest error rate on the **test** data?  Provide both the hyperparameter value and the test error rate?   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6413b2d0273f00fc600e4810f1743b1",
     "grade": true,
     "grade_id": "cell-6556a7e8842e725c",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## *YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Question 5</span>\n",
    "\n",
    "* Create a plot with number of leaves vs. error rates (both training and test) varying the maximum number of leaves from 2 to 25.\n",
    "\n",
    "* Do you think *this* error rate will be reflective of how well our model will perform on unseen data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "523208658e0520335f40b6b98f5a0372",
     "grade": true,
     "grade_id": "cell-954338e7a6b2069d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write your code to generate your plot here for Question 5\n",
    "\"\"\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1642077c20385278a4c453a81aa7162c",
     "grade": false,
     "grade_id": "cell-a83b9b0a7eba238e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# <span style=\"color:red\">Question 6</span>\n",
    "\n",
    "* Do you think *this* error rate will be reflective of how well our model will perform on unseen data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19bad90b45b6064a8f2735ff82dae9b6",
     "grade": true,
     "grade_id": "cell-c1f723fd33396e77",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## *YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1edf1ddb0dd05ebdae3c03531187597",
     "grade": false,
     "grade_id": "cell-9c1e4c7ba88d0421",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# A Danger in ML -- Model Overfitting\n",
    "\n",
    "Discussed in IDD Chapter 3.4.\n",
    "\n",
    "Looking at the model's error_rate over both the training set and testing set as a function of the maximum number of leaves offers a lot of insight.\n",
    "\n",
    "When the number of leaves is low (2 for example), we get a very simple model (can't get simplier than two leaves).  When both the training and test error rates are high, we characerize the model as **underfitting** the data.  The model simply does not have enough parameters to capture the relationship between the features and the class labels. \n",
    "\n",
    "When the number of leaves is very high, we get a complex model.  The tree for this model has \n",
    "many decisions/branches, and in the extreme case, can have leaves with one a single example\n",
    "from the training set.  So, you might ask, how can a model generalize if the leaves \n",
    "don't contain more than one example?  The answer in general, is that **is can not**.\n",
    "\n",
    "One way of the thinking about a model like this is that it is memorizing the\n",
    "answers instead of learning the relationship between the features and the class labels.  This becomes\n",
    "apparent when, given new data (the test set), the model does not perform as well as on the training\n",
    "data.  This phenomena is known as **model overfitting** and is a very important concept, as it has\n",
    "lead to some of the worst failures in the application of machine learning.  \n",
    "\n",
    "The tool we have at this point to identify underfitting and overfitting is the plot \n",
    "of the error rate on the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da065da8c35e34e93bc5f099852e8b48",
     "grade": false,
     "grade_id": "cell-62ec1d18267e2c8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Generalization Error \n",
    "\n",
    "Notice that in this example we are using our test set for *both* model selection and model evaluation.  We used it for **model selection** by searching for a hyperparameter setting that minimizes error on the test set.  We use it for **model evaluation** by using our test set error as an estimate of the expected error rate on unobserved data.\n",
    "\n",
    "Let's see how our model does on some new, unobserved data drawn from the same distribution.  This cell will give us a good estimate of our *actual* generalization error.  (Note that in real-world problems we can't run a test like this because we don't have unlimited access to extra data that we can use to check our work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_leaf_nodes=????) # Put your best hyperparameter here!\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Let's see how we do on unobserved data... \n",
    "X_new, y_new = make_moons(n_samples=200, noise=0.3, random_state=2)\n",
    "y_new_predict = tree.predict(X_new)\n",
    "error_rate = np.sum((y_new - y_new_predict)**2) / y_new.size\n",
    "print(\"Error rate: {:.4f}\".format(error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c5cfc3696a8befbe879966c953549cf",
     "grade": false,
     "grade_id": "cell-3a103bc2683741c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# <span style=\"color:red\">Question 7</span>\n",
    "\n",
    "* Relative to Activity 1, where we just looked for the model that best fit our training data, would you say that our train/test split was beneficial in terms of model selection, i.e. did we end up with a better model? Justify your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Answer to 7</span>\n",
    "Your answer goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "543558347189adef62d6400ec434847c",
     "grade": false,
     "grade_id": "cell-8a4f665ff58145be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# <span style=\"color:red\">Question 8</span>\n",
    "* Would you say that our train/test split was beneficial in terms of model evaluation, i.e. were we able make a better prediction of our generalization error?  Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e22e90d0ea0443034fdc74e82bb8d6e1",
     "grade": true,
     "grade_id": "cell-d024e59d62785d9f",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## *YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cef99c47f8988a71d36fae37adcb116",
     "grade": false,
     "grade_id": "cell-82fa29d6eb6f8420",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# <span style=\"color:red\">Question 9</span>  \n",
    "\n",
    "Do you see any problems here in terms of model selection or evaluation?  How accurate was our prediction of generalization error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3f35af96e21e79b6ce5cd23e98e3cd7",
     "grade": true,
     "grade_id": "cell-faaa824dd742dd94",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## *YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Click [here](model_selection_classification_2.ipynb) to open the next page of exercises..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2eed6381e3275af77f742cc768b57fa22de6718e88095d2c7dc88ebc99d5d688"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
